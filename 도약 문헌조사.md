# 문헌조사

## Forest Learning From Data and Its universal Coding

데이터가 불충분 할 때,

- 주어진 데이터의 posterior probability를 maximize하는 forest 모델
- 실제 데이터의 샘플 n이 증가 할 때 실제 값에 수렴하는 forest 모델

의 차이점을 보이고, 둘 중 하나를 선택하는 알고리즘을 구성함

즉, n sample of p variables에서 pn values 가 빠져있을때(missing) posterior probability를 최대화 하는 모델을 찾음.

- 이 논문에서 모델이 베이시안 네트워크 일 때 missing variables이 많아 질 수록 계산량이 크게 증가하기 때문에 모델을 forest로 가정 함.
- 이 논문에서 제안하는 posterior probability를 maximize하는 모델은 계산량을 증가시키지 않음.

또한, sample n이 무한대로 증가해도 variable p의  data가 불완전(missing)하다면,  estimated bayes optimal model이 true값에 수렴하지 않는다는걸 이 논문에서 제안한 모델로 보임.

## Instance-dependent $l_{\infty}$-bound for policy evaluation in tabular reinforcement learning

Finite state를 가지고 있는 discounted markov reward processes(MRPs)가 무한히 반복 될 때 infinity-norm에 대한 연구

- Discounted MRPs의 Instance-dependent 한 upper bound와 MRP의 minimax complexity lower bounds를 제시함.

두가지 종류에 대한 결과를 보임

- 관측된 데이터에 대한 high-probability upper bound
- 알려지지 않은 population transition martix와 reward function에 대한 upper bound

